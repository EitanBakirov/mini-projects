{
  "2412.05579v2": {
    "title": "LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods",
    "authors": [
      "Haitao Li",
      "Qian Dong",
      "Junjie Chen",
      "Huixue Su",
      "Yujia Zhou",
      "Qingyao Ai",
      "Ziyi Ye",
      "Yiqun Liu"
    ],
    "summary": "The rapid advancement of Large Language Models (LLMs) has driven their\nexpanding application across various fields. One of the most promising\napplications is their role as evaluators based on natural language responses,\nreferred to as ''LLMs-as-judges''. This framework has attracted growing\nattention from both academia and industry due to their excellent effectiveness,\nability to generalize across tasks, and interpretability in the form of natural\nlanguage. This paper presents a comprehensive survey of the LLMs-as-judges\nparadigm from five key perspectives: Functionality, Methodology, Applications,\nMeta-evaluation, and Limitations. We begin by providing a systematic definition\nof LLMs-as-Judges and introduce their functionality (Why use LLM judges?). Then\nwe address methodology to construct an evaluation system with LLMs (How to use\nLLM judges?). Additionally, we investigate the potential domains for their\napplication (Where to use LLM judges?) and discuss methods for evaluating them\nin various contexts (How to evaluate LLM judges?). Finally, we provide a\ndetailed analysis of the limitations of LLM judges and discuss potential future\ndirections. Through a structured and comprehensive analysis, we aim aims to\nprovide insights on the development and application of LLMs-as-judges in both\nresearch and practice. We will continue to maintain the relevant resource list\nat https://github.com/CSHaitao/Awesome-LLMs-as-Judges.",
    "pdf_url": "http://arxiv.org/pdf/2412.05579v2",
    "published": "2024-12-07"
  },
  "2412.09569v1": {
    "title": "JuStRank: Benchmarking LLM Judges for System Ranking",
    "authors": [
      "Ariel Gera",
      "Odellia Boni",
      "Yotam Perlitz",
      "Roy Bar-Haim",
      "Lilach Eden",
      "Asaf Yehudai"
    ],
    "summary": "Given the rapid progress of generative AI, there is a pressing need to\nsystematically compare and choose between the numerous models and\nconfigurations available. The scale and versatility of such evaluations make\nthe use of LLM-based judges a compelling solution for this challenge.\nCrucially, this approach requires first to validate the quality of the LLM\njudge itself. Previous work has focused on instance-based assessment of LLM\njudges, where a judge is evaluated over a set of responses, or response pairs,\nwhile being agnostic to their source systems. We argue that this setting\noverlooks critical factors affecting system-level ranking, such as a judge's\npositive or negative bias towards certain systems. To address this gap, we\nconduct the first large-scale study of LLM judges as system rankers. System\nscores are generated by aggregating judgment scores over multiple system\noutputs, and the judge's quality is assessed by comparing the resulting system\nranking to a human-based ranking. Beyond overall judge assessment, our analysis\nprovides a fine-grained characterization of judge behavior, including their\ndecisiveness and bias.",
    "pdf_url": "http://arxiv.org/pdf/2412.09569v1",
    "published": "2024-12-12"
  },
  "2408.13006v2": {
    "title": "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
    "authors": [
      "Hui Wei",
      "Shenghua He",
      "Tian Xia",
      "Fei Liu",
      "Andy Wong",
      "Jingyang Lin",
      "Mei Han"
    ],
    "summary": "LLM-as-a-Judge has been widely applied to evaluate and compare different LLM\nalignmnet approaches (e.g., RLHF and DPO). However, concerns regarding its\nreliability have emerged, due to LLM judges' biases and inconsistent\ndecision-making. Previous research has developed evaluation frameworks to\nassess reliability of LLM judges and their alignment with human preferences.\nHowever, the employed evaluation metrics often lack adequate explainability and\nfail to address LLM internal inconsistency. Additionally, existing studies\ninadequately explore the impact of various prompt templates when applying\nLLM-as-a-Judge methods, leading to potentially inconsistent comparisons between\ndifferent alignment algorithms. In this work, we systematically evaluate\nLLM-as-a-Judge on alignment tasks by defining more theoretically interpretable\nevaluation metrics and explicitly mitigating LLM internal inconsistency from\nreliability metrics. We develop an open-source framework to evaluate, compare,\nand visualize the reliability and alignment of LLM judges, which facilitates\npractitioners to choose LLM judges for alignment tasks. In the experiments, we\nexamine effects of diverse prompt templates on LLM-judge reliability and also\ndemonstrate our developed framework by comparing various LLM judges on two\ncommon alignment datasets (i.e., TL;DR Summarization and HH-RLHF-Helpfulness).\nOur results indicate a significant impact of prompt templates on LLM judge\nperformance, as well as a mediocre alignment level between the tested LLM\njudges and human evaluators.",
    "pdf_url": "http://arxiv.org/pdf/2408.13006v2",
    "published": "2024-08-23"
  }
}