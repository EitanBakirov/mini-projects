{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e77535a4",
   "metadata": {},
   "source": [
    "# Function Calling Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cfd1f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: arxiv in c:\\users\\eitan\\onedrive\\desktop\\eitan\\projects\\neural nets\\.conda\\lib\\site-packages (2.2.0)\n",
      "Requirement already satisfied: dotenv in c:\\users\\eitan\\onedrive\\desktop\\eitan\\projects\\neural nets\\.conda\\lib\\site-packages (0.9.9)\n",
      "Requirement already satisfied: anthropic in c:\\users\\eitan\\onedrive\\desktop\\eitan\\projects\\neural nets\\.conda\\lib\\site-packages (0.52.0)\n",
      "Requirement already satisfied: feedparser~=6.0.10 in c:\\users\\eitan\\onedrive\\desktop\\eitan\\projects\\neural nets\\.conda\\lib\\site-packages (from arxiv) (6.0.11)\n",
      "Requirement already satisfied: requests~=2.32.0 in c:\\users\\eitan\\appdata\\roaming\\python\\python311\\site-packages (from arxiv) (2.32.3)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\eitan\\onedrive\\desktop\\eitan\\projects\\neural nets\\.conda\\lib\\site-packages (from dotenv) (1.1.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\eitan\\onedrive\\desktop\\eitan\\projects\\neural nets\\.conda\\lib\\site-packages (from anthropic) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\eitan\\onedrive\\desktop\\eitan\\projects\\neural nets\\.conda\\lib\\site-packages (from anthropic) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.25.0 in c:\\users\\eitan\\onedrive\\desktop\\eitan\\projects\\neural nets\\.conda\\lib\\site-packages (from anthropic) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\eitan\\onedrive\\desktop\\eitan\\projects\\neural nets\\.conda\\lib\\site-packages (from anthropic) (0.9.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\eitan\\onedrive\\desktop\\eitan\\projects\\neural nets\\.conda\\lib\\site-packages (from anthropic) (2.11.3)\n",
      "Requirement already satisfied: sniffio in c:\\users\\eitan\\onedrive\\desktop\\eitan\\projects\\neural nets\\.conda\\lib\\site-packages (from anthropic) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in c:\\users\\eitan\\onedrive\\desktop\\eitan\\projects\\neural nets\\.conda\\lib\\site-packages (from anthropic) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\eitan\\onedrive\\desktop\\eitan\\projects\\neural nets\\.conda\\lib\\site-packages (from anyio<5,>=3.5.0->anthropic) (3.7)\n",
      "Requirement already satisfied: sgmllib3k in c:\\users\\eitan\\onedrive\\desktop\\eitan\\projects\\neural nets\\.conda\\lib\\site-packages (from feedparser~=6.0.10->arxiv) (1.0.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\eitan\\onedrive\\desktop\\eitan\\projects\\neural nets\\.conda\\lib\\site-packages (from httpx<1,>=0.25.0->anthropic) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\eitan\\onedrive\\desktop\\eitan\\projects\\neural nets\\.conda\\lib\\site-packages (from httpx<1,>=0.25.0->anthropic) (1.0.8)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\eitan\\onedrive\\desktop\\eitan\\projects\\neural nets\\.conda\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.25.0->anthropic) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\eitan\\onedrive\\desktop\\eitan\\projects\\neural nets\\.conda\\lib\\site-packages (from pydantic<3,>=1.9.0->anthropic) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in c:\\users\\eitan\\onedrive\\desktop\\eitan\\projects\\neural nets\\.conda\\lib\\site-packages (from pydantic<3,>=1.9.0->anthropic) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\eitan\\onedrive\\desktop\\eitan\\projects\\neural nets\\.conda\\lib\\site-packages (from pydantic<3,>=1.9.0->anthropic) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\eitan\\onedrive\\desktop\\eitan\\projects\\neural nets\\.conda\\lib\\site-packages (from requests~=2.32.0->arxiv) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\eitan\\onedrive\\desktop\\eitan\\projects\\neural nets\\.conda\\lib\\site-packages (from requests~=2.32.0->arxiv) (2.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install arxiv dotenv anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "513bb01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "import json\n",
    "import os\n",
    "from typing import List\n",
    "from dotenv import load_dotenv\n",
    "import anthropic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10772aac",
   "metadata": {},
   "source": [
    "## Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70f3bc9",
   "metadata": {},
   "source": [
    "### Tool Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d373912",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAPER_DIR = \"papers\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5023810b",
   "metadata": {},
   "source": [
    "The first tool searches for relevant arXiv papers based on a topic and stores the papers' info in a JSON file (title, authors, summary, paper url and the publication date). The JSON files are organized by topics in the `papers` directory. The tool does not download the papers.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b3f9732",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_papers(topic: str, max_results: int = 5) -> List[str]:\n",
    "    \"\"\"\n",
    "    Search for papers on arXiv based on a topic and store their information.\n",
    "    \n",
    "    Args:\n",
    "        topic: The topic to search for\n",
    "        max_results: Maximum number of results to retrieve (default: 5)\n",
    "        \n",
    "    Returns:\n",
    "        List of paper IDs found in the search\n",
    "    \"\"\"\n",
    "    \n",
    "    # Use arxiv to find the papers \n",
    "    client = arxiv.Client()\n",
    "\n",
    "    # Search for the most relevant articles matching the queried topic\n",
    "    search = arxiv.Search(\n",
    "        query = topic,\n",
    "        max_results = max_results,\n",
    "        sort_by = arxiv.SortCriterion.Relevance\n",
    "    )\n",
    "\n",
    "    papers = client.results(search)\n",
    "    \n",
    "    # Create directory for this topic\n",
    "    path = os.path.join(PAPER_DIR, topic.lower().replace(\" \", \"_\"))\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    \n",
    "    file_path = os.path.join(path, \"papers_info.json\")\n",
    "\n",
    "    # Try to load existing papers info\n",
    "    try:\n",
    "        with open(file_path, \"r\") as json_file:\n",
    "            papers_info = json.load(json_file)\n",
    "    except (FileNotFoundError, json.JSONDecodeError):\n",
    "        papers_info = {}\n",
    "\n",
    "    # Process each paper and add to papers_info  \n",
    "    paper_ids = []\n",
    "    for paper in papers:\n",
    "        paper_ids.append(paper.get_short_id())\n",
    "        paper_info = {\n",
    "            'title': paper.title,\n",
    "            'authors': [author.name for author in paper.authors],\n",
    "            'summary': paper.summary,\n",
    "            'pdf_url': paper.pdf_url,\n",
    "            'published': str(paper.published.date())\n",
    "        }\n",
    "        papers_info[paper.get_short_id()] = paper_info\n",
    "    \n",
    "    # Save updated papers_info to json file\n",
    "    with open(file_path, \"w\") as json_file:\n",
    "        json.dump(papers_info, json_file, indent=2)\n",
    "    \n",
    "    print(f\"Results are saved in: {file_path}\")\n",
    "    \n",
    "    return paper_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee14ef58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results are saved in: papers\\computers\\papers_info.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['1310.7911v2',\n",
       " 'math/9711204v1',\n",
       " '2208.00733v1',\n",
       " '2504.07020v1',\n",
       " '2403.03925v1']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_papers(\"computers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74425b0",
   "metadata": {},
   "source": [
    "The second tool looks for information about a specific paper across all topic directories inside the `papers` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e952cb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_info(paper_id: str) -> str:\n",
    "    \"\"\"\n",
    "    Search for information about a specific paper across all topic directories.\n",
    "    \n",
    "    Args:\n",
    "        paper_id: The ID of the paper to look for\n",
    "        \n",
    "    Returns:\n",
    "        JSON string with paper information if found, error message if not found\n",
    "    \"\"\"\n",
    " \n",
    "    for item in os.listdir(PAPER_DIR):\n",
    "        item_path = os.path.join(PAPER_DIR, item)\n",
    "        if os.path.isdir(item_path):\n",
    "            file_path = os.path.join(item_path, \"papers_info.json\")\n",
    "            if os.path.isfile(file_path):\n",
    "                try:\n",
    "                    with open(file_path, \"r\") as json_file:\n",
    "                        papers_info = json.load(json_file)\n",
    "                        if paper_id in papers_info:\n",
    "                            return json.dumps(papers_info[paper_id], indent=2)\n",
    "                except (FileNotFoundError, json.JSONDecodeError) as e:\n",
    "                    print(f\"Error reading {file_path}: {str(e)}\")\n",
    "                    continue\n",
    "    \n",
    "    return f\"There's no saved information related to paper {paper_id}.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f292f2eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n  \"title\": \"Compact manifolds with computable boundaries\",\\n  \"authors\": [\\n    \"Zvonko Iljazovic\"\\n  ],\\n  \"summary\": \"We investigate conditions under which a co-computably enumerable closed set\\\\nin a computable metric space is computable and prove that in each locally\\\\ncomputable computable metric space each co-computably enumerable compact\\\\nmanifold with computable boundary is computable. In fact, we examine the notion\\\\nof a semi-computable compact set and we prove a more general result: in any\\\\ncomputable metric space each semi-computable compact manifold with computable\\\\nboundary is computable. In particular, each semi-computable compact\\\\n(boundaryless) manifold is computable.\",\\n  \"pdf_url\": \"http://arxiv.org/pdf/1310.7911v2\",\\n  \"published\": \"2013-10-29\"\\n}'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_info('1310.7911v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e6bfc7",
   "metadata": {},
   "source": [
    "### Tool Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "628ab3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    {\n",
    "        \"name\": \"search_papers\",\n",
    "        \"description\": \"Search for papers on arXiv based on a topic and store their information.\",\n",
    "        \"input_schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"topic\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The topic to search for\"\n",
    "                }, \n",
    "                \"max_results\": {\n",
    "                    \"type\": \"integer\",\n",
    "                    \"description\": \"Maximum number of results to retrieve\",\n",
    "                    \"default\": 5\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"topic\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"extract_info\",\n",
    "        \"description\": \"Search for information about a specific paper across all topic directories.\",\n",
    "        \"input_schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"paper_id\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The ID of the paper to look for\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"paper_id\"]\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8389dad0",
   "metadata": {},
   "source": [
    "### Tool Mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6847024c",
   "metadata": {},
   "source": [
    "This code handles tool mapping and execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dbfadbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_tool_function = {\n",
    "    \"search_papers\": search_papers,\n",
    "    \"extract_info\": extract_info\n",
    "}\n",
    "\n",
    "def execute_tool(tool_name, tool_args):\n",
    "    \n",
    "    result = mapping_tool_function[tool_name](**tool_args)\n",
    "\n",
    "    if result is None:\n",
    "        result = \"The operation completed but didn't return any results.\"\n",
    "        \n",
    "    elif isinstance(result, list):\n",
    "        result = ', '.join(result)\n",
    "        \n",
    "    elif isinstance(result, dict):\n",
    "        # Convert dictionaries to formatted JSON strings\n",
    "        result = json.dumps(result, indent=2)\n",
    "    \n",
    "    else:\n",
    "        # For any other type, convert using str()\n",
    "        result = str(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397fa575",
   "metadata": {},
   "source": [
    "### Chatbot Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868e0d7c",
   "metadata": {},
   "source": [
    "The chatbot handles the user's queries one by one, but it does not persist memory across the queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31f4d93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(\"../\") \n",
    "client = anthropic.Anthropic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d4a5edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import openai\n",
    "# load_dotenv() \n",
    "# client = openai.Client(api_key=os.getenv('OPENAI_API_KEY'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d06262f",
   "metadata": {},
   "source": [
    "### Query Processing   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89ab1535",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_query(query):\n",
    "    \n",
    "    messages = [{'role': 'user', 'content': query}]\n",
    "    \n",
    "    response = client.messages.create(max_tokens = 2024,\n",
    "                                  model = 'claude-3-7-sonnet-20250219', \n",
    "                                  tools = tools,\n",
    "                                  messages = messages)\n",
    "    \n",
    "    \n",
    "    process_query = True\n",
    "    while process_query:\n",
    "        assistant_content = []\n",
    "\n",
    "        for content in response.content:\n",
    "            if content.type == 'text':\n",
    "                \n",
    "                print(content.text)\n",
    "                assistant_content.append(content)\n",
    "                \n",
    "                if len(response.content) == 1:\n",
    "                    process_query = False\n",
    "            \n",
    "            elif content.type == 'tool_use':\n",
    "                \n",
    "                assistant_content.append(content)\n",
    "                messages.append({'role': 'assistant', 'content': assistant_content})\n",
    "                \n",
    "                tool_id = content.id\n",
    "                tool_args = content.input\n",
    "                tool_name = content.name\n",
    "                print(f\"Calling tool {tool_name} with args {tool_args}\")\n",
    "                \n",
    "                result = execute_tool(tool_name, tool_args)\n",
    "                messages.append({\"role\": \"user\", \n",
    "                                  \"content\": [\n",
    "                                      {\n",
    "                                          \"type\": \"tool_result\",\n",
    "                                          \"tool_use_id\": tool_id,\n",
    "                                          \"content\": result\n",
    "                                      }\n",
    "                                  ]\n",
    "                                })\n",
    "                response = client.messages.create(max_tokens = 2024,\n",
    "                                  model = 'claude-3-7-sonnet-20250219', \n",
    "                                  tools = tools,\n",
    "                                  messages = messages) \n",
    "                \n",
    "                if len(response.content) == 1 and response.content[0].type == \"text\":\n",
    "                    print(response.content[0].text)\n",
    "                    process_query = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd9ee53",
   "metadata": {},
   "source": [
    "### Chat Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "248ae203",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_loop():\n",
    "    print(\"Type your queries or 'quit' to exit.\")\n",
    "    while True:\n",
    "        try:\n",
    "            query = input(\"\\nQuery: \").strip()\n",
    "            print(\"User query:\", query)\n",
    "            # Exit the loop if the user types 'quit'\n",
    "            if query.lower() == 'quit':\n",
    "                break\n",
    "            \n",
    "            process_query(query)\n",
    "            print(\"\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9da41908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type your queries or 'quit' to exit.\n",
      "User query: Hi\n",
      "Hello! I'm here to help you search for research papers on arXiv or extract information about specific papers. What topic would you like to explore or which paper would you like to learn more about?\n",
      "\n",
      "To help you, I can:\n",
      "1. Search for papers on a particular topic\n",
      "2. Extract detailed information about a specific paper using its arXiv ID\n",
      "\n",
      "Please let me know what you're interested in, and I'll assist you further.\n",
      "\n",
      "\n",
      "User query: LLM agents\n",
      "I'd be happy to help you find research papers about LLM agents. Let me search for papers on this topic on arXiv for you.\n",
      "Calling tool search_papers with args {'topic': 'LLM agents', 'max_results': 5}\n",
      "Results are saved in: papers\\llm_agents\\papers_info.json\n",
      "I've found 5 papers related to LLM agents. Let me gather more detailed information about each of these papers for you:\n",
      "Calling tool extract_info with args {'paper_id': '2503.19213v1'}\n",
      "Calling tool extract_info with args {'paper_id': '2408.12680v2'}\n",
      "Calling tool extract_info with args {'paper_id': '2410.13919v2'}\n",
      "Calling tool extract_info with args {'paper_id': '2505.09396v1'}\n",
      "Calling tool extract_info with args {'paper_id': '2408.02479v2'}\n",
      "Based on my search, here are 5 recent papers about LLM agents:\n",
      "\n",
      "1. **A Survey of Large Language Model Agents for Question Answering** (March 2025)\n",
      "   - Explores how LLM-based agents improve question answering capabilities\n",
      "   - Discusses planning, question understanding, information retrieval, and answer generation stages\n",
      "   - Identifies ongoing challenges and future research directions\n",
      "\n",
      "2. **Can LLMs Understand Social Norms in Autonomous Driving Games?** (August 2024)\n",
      "   - Investigates LLM agents in autonomous driving scenarios\n",
      "   - Shows LLM agents can handle dynamic environments in Markov games\n",
      "   - Demonstrates emergence of social norms in multi-agent systems\n",
      "\n",
      "3. **LLM Agent Honeypot: Monitoring AI Hacking Agents in the Wild** (October 2024)\n",
      "   - Presents a system to monitor autonomous AI hacking agents\n",
      "   - Tracked over 8 million hacking attempts over 3 months\n",
      "   - Identified 8 potential AI agents in real-world attacks\n",
      "\n",
      "4. **The Influence of Human-inspired Agentic Sophistication in LLM-driven Strategic Reasoners** (May 2025)\n",
      "   - Examines how human-inspired cognitive structures enhance LLM agents\n",
      "   - Tests different agent designs in guessing games against human participants\n",
      "   - Finds the relationship between design complexity and human-likeness is non-linear\n",
      "\n",
      "5. **From LLMs to LLM-based Agents for Software Engineering: A Survey** (August 2024)\n",
      "   - Comprehensive survey of LLM agents in software engineering\n",
      "   - Covers six key topics including requirement engineering, code generation, and autonomous decision-making\n",
      "   - Differentiates between LLMs and LLM-based agents in various software engineering tasks\n",
      "\n",
      "Would you like more detailed information about any specific paper or aspect of LLM agents?\n",
      "\n",
      "\n",
      "User query: 2503.19213v1\n",
      "I'll help you get information about this paper. The input you've provided appears to be an arXiv paper ID. Let me search for details about this specific paper.\n",
      "Calling tool extract_info with args {'paper_id': '2503.19213v1'}\n",
      "## Paper Information: \"A Survey of Large Language Model Agents for Question Answering\"\n",
      "\n",
      "### Key Details:\n",
      "- **Paper ID**: 2503.19213v1\n",
      "- **Title**: A Survey of Large Language Model Agents for Question Answering\n",
      "- **Author**: Murong Yue\n",
      "- **Published Date**: March 24, 2025\n",
      "- **PDF URL**: [http://arxiv.org/pdf/2503.19213v1](http://arxiv.org/pdf/2503.19213v1)\n",
      "\n",
      "### Summary:\n",
      "This paper presents a comprehensive survey on large language model (LLM)-based agents specifically designed for question answering (QA) tasks. The research highlights how LLM-based agents overcome limitations faced by traditional agents, such as heavy data requirements and poor generalization to new environments.\n",
      "\n",
      "The survey systematically reviews the design of LLM agents for QA across four key stages:\n",
      "1. Planning\n",
      "2. Question understanding\n",
      "3. Information retrieval\n",
      "4. Answer generation\n",
      "\n",
      "The paper demonstrates that LLM-based agents achieve better QA results compared to traditional QA pipelines and basic LLM QA systems by enabling interaction with external environments. It also identifies current challenges and explores future research directions for improving LLM agent QA systems.\n",
      "\n",
      "\n",
      "User query: quit\n"
     ]
    }
   ],
   "source": [
    "chat_loop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
